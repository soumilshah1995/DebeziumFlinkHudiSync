{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af849dec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: apache-flink\r\n",
      "Version: 1.19.0\r\n",
      "Summary: Apache Flink Python API\r\n",
      "Home-page: https://flink.apache.org\r\n",
      "Author: Apache Software Foundation\r\n",
      "Author-email: dev@flink.apache.org\r\n",
      "License: https://www.apache.org/licenses/LICENSE-2.0\r\n",
      "Location: /opt/homebrew/lib/python3.11/site-packages\r\n",
      "Requires: apache-beam, apache-flink-libraries, avro-python3, cloudpickle, fastavro, httplib2, numpy, pandas, pemja, protobuf, py4j, pyarrow, python-dateutil, pytz, requests, ruamel.yaml\r\n",
      "Required-by: \r\n"
     ]
    }
   ],
   "source": [
    "!pip show apache-flink"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b853805",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyflink.common.configuration.Configuration at 0x134bf3790>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.environ['JAVA_HOME']='/opt/homebrew/opt/openjdk@11'\n",
    "\n",
    "\n",
    "from pyflink.table import EnvironmentSettings, TableEnvironment\n",
    "\n",
    "# Create a batch TableEnvironment\n",
    "env_settings = EnvironmentSettings.new_instance().in_streaming_mode().build()\n",
    "table_env = TableEnvironment.create(env_settings)\n",
    "\n",
    "# Get the current working directory\n",
    "CURRENT_DIR = os.getcwd()\n",
    "\n",
    "# Define a list of JAR file names you want to add\n",
    "jar_files = [\n",
    "    \"flink-jar/flink-sql-avro-confluent-registry-1.19.0.jar\",\n",
    "    \"flink-jar/flink-sql-connector-kafka-3.1.0-1.18.jar\"\n",
    "    \n",
    "    \n",
    "]\n",
    "\n",
    "\n",
    "# Build the list of JAR URLs by prepending 'file:///' to each file name\n",
    "jar_urls = [f\"file:///{CURRENT_DIR}/{jar_file}\" for jar_file in jar_files]\n",
    "\n",
    "table_env.get_config().get_configuration().set_string(\n",
    "    \"pipeline.jars\",\n",
    "    \";\".join(jar_urls)\n",
    ")\n",
    "table_env.get_config().get_configuration().set_string(\n",
    "    \"execution.checkpointing.interval\",\n",
    "    \"5000\"\n",
    ")\n",
    "\n",
    "table_env.get_config().get_configuration().set_string(\n",
    "    \"parallelism.default\",\n",
    "    \"4\"\n",
    ")\n",
    "\n",
    "# Configure checkpointing\n",
    "table_env.get_config().get_configuration().set_string(\n",
    "    \"execution.checkpointing.mode\",\n",
    "    \"EXACTLY_ONCE\"\n",
    ")\n",
    "\n",
    "# Set the checkpointing directory to the current directory\n",
    "table_env.get_config().get_configuration().set_string(\n",
    "    \"execution.checkpointing.checkpoints-directory\",\n",
    "    CURRENT_DIR\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac8b53b9",
   "metadata": {},
   "source": [
    "# Create Source  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e62a266d",
   "metadata": {},
   "source": [
    "#### Customer Source "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6038a806",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created customer_source table.\n"
     ]
    }
   ],
   "source": [
    "customer_source = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS customer_source (\n",
    "  id STRING,\n",
    "    first_name STRING,\n",
    "    last_name STRING,\n",
    "    email STRING\n",
    ") WITH (\n",
    "    'connector' = 'kafka',\n",
    "    'topic' = 'inventory.inventory.customers',\n",
    "    'properties.bootstrap.servers' = 'localhost:7092',\n",
    "    'properties.group.id' = 'sales',\n",
    "    'scan.startup.mode' = 'earliest-offset',\n",
    "    'format' = 'debezium-json'\n",
    ")\n",
    "\"\"\"\n",
    "# Execute the SQL to create the sources\n",
    "table_env.execute_sql(customer_source)\n",
    "print(\"Created customer_source table.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e0ec86",
   "metadata": {},
   "source": [
    "#### Order Source "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "378274cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created orders_source table.\n"
     ]
    }
   ],
   "source": [
    "orders_source = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS product_source (\n",
    "  order_number STRING,\n",
    "  order_date STRING,\n",
    "  purchaser STRING,\n",
    "  quantity STRING,\n",
    "  product_id STRING\n",
    ") WITH (\n",
    "    'connector' = 'kafka',\n",
    "    'topic' = 'inventory.inventory.orders',\n",
    "    'properties.bootstrap.servers' = 'localhost:7092',\n",
    "    'properties.group.id' = 'order_group',\n",
    "    'scan.startup.mode' = 'earliest-offset',\n",
    "    'format' = 'debezium-json'\n",
    ")\n",
    "\"\"\"\n",
    "# Execute the SQL to create the sources\n",
    "table_env.execute_sql(orders_source)\n",
    "print(\"Created orders_source table.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce5a339e",
   "metadata": {},
   "source": [
    "# Join Two Streams "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6077aa02",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# join_query = \"\"\"\n",
    "# SELECT \n",
    "#     p.order_number,\n",
    "#     p.order_date,\n",
    "#     c.id AS customer_id,\n",
    "#     c.first_name,\n",
    "#     c.last_name,\n",
    "#     c.email,\n",
    "#     p.quantity,\n",
    "#     p.product_id\n",
    "# FROM product_source AS p\n",
    "# JOIN customer_source AS c ON p.purchaser = c.id\n",
    "# \"\"\"\n",
    "\n",
    "\n",
    "# # Execute the join query\n",
    "# result = table_env.execute_sql(join_query)\n",
    "# result.print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd7f1e80",
   "metadata": {},
   "source": [
    "# Sink"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7652edf3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyflink.table.table_result.TableResult at 0x134c17b10>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orders_sink = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS kafka_upsert_sink (\n",
    "    order_number STRING,\n",
    "    order_date STRING,\n",
    "    customer_id STRING,\n",
    "    first_name STRING,\n",
    "    last_name STRING,\n",
    "    email STRING,\n",
    "    quantity STRING,\n",
    "    product_id STRING,\n",
    "    PRIMARY KEY (order_number) NOT ENFORCED\n",
    ") WITH (\n",
    "    'connector' = 'upsert-kafka',\n",
    "    'topic' = 'processed_orders',\n",
    "    'properties.bootstrap.servers' = 'localhost:7092',\n",
    "  'key.format' = 'avro-confluent',\n",
    "  'value.format' = 'avro-confluent',\n",
    "  'key.avro-confluent.url' = 'http://localhost:8081',\n",
    "  'value.avro-confluent.url' = 'http://localhost:8081'\n",
    ")\n",
    "\"\"\"\n",
    "table_env.execute_sql(orders_sink)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "386d22ee",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Insert joined data into the Kafka sink\n",
    "join_and_insert_query = \"\"\"\n",
    "INSERT INTO kafka_upsert_sink\n",
    "SELECT \n",
    "    p.order_number,\n",
    "    p.order_date,\n",
    "    c.id AS customer_id,\n",
    "    c.first_name,\n",
    "    c.last_name,\n",
    "    c.email,\n",
    "    p.quantity,\n",
    "    p.product_id\n",
    "FROM product_source AS p\n",
    "JOIN customer_source AS c ON p.purchaser = c.id\n",
    "\"\"\"\n",
    "table_env.execute_sql(join_and_insert_query).wait()\n",
    "\n",
    "print(\"Streaming data to Kafka topic 'processed_orders'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b35e22f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
